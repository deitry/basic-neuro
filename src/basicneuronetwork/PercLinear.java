/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */
package basicneuronetwork;

/**
 * 
 * @author Дима
 * Класс будет реализовывать перцептрон с линейной функцией активации.
 * На его основе попробую отработать концепцию "разных" нейронов.
 * В будущем планируется введение также экспоненциальных, синусоидальных
 * и полиномиальных элементов.
 * 
 * TODO : чтобы класс полностью обеспечивал реализацию нейронов с различными
 * функциями активации, нужно, чтобы производная (необходимая для вычисления
 * ошибки) также обеспечивалась методами класса. Итого нужно:
 *   1) перегрузить getOutput с соответствующей заложенной характеристикой
 *   2) встроить в класс функцию getGradient (?) - будет выдавать значение
 *      "градиента ошибки"
 *      Общая схема поправки к весам: delta w = - eta * d E / d w(i,j)
 *      Возможно, имеет смысл вообще встроить процедуру корректировки весов
 *      как один из методов класса Perceptron; так можно будет легко
 *      корректировать скорость обучения для каждого отдельного нейрона плюс
 *      это даст независимость сети от варианта реализации функции активации.
 *      К слову сказать, выражение "функция активации" имеет смысл заменить на
 *      "функцию, реализуемую нейроном" или просто "функцию", т.к. планируется
 *      сделать несколько таких классов как замену крупных "предварительно
 *      отработанных" нейросетей для вычисления простых функций.
 *      Теоретически, это даст большое ускорение в обучении. А может и нет.
 */
public class PercLinear extends Perceptron {
    private double[] weights;
    /**
     *
     * @param inputCount
     */
    public PercLinear(int inputCount) {
        super(inputCount);
        weights = new double[inputCount];
        // заполняем произвольными весами
        for (int i = 0; i < weights.length; i++) {
            weights[i] = Math.random();
        }
    }
}
